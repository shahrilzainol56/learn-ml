{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821b7915-086d-4b16-8751-29a1ae2a3059",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6e435-f19f-4237-ac70-81938fe5f49d",
   "metadata": {},
   "source": [
    "Let's build a voting classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7abff-716d-43a0-8a7a-f2c0c91d9d91",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a VotingClassifier class that’s quite easy to use: just give\n",
    "it a list of name/predictor pairs, and use it like a normal classifier. Let’s try it on\n",
    "the moons dataset. We will load and split the moons\n",
    "dataset into a training set and a test set, then we’ll create and train a voting classifier\n",
    "composed of three diverse classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd8c1bf-4811-4654-8cdf-50bcfeb8913d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ecfd1-754e-4f92-9867-a9e6546c9a2b",
   "metadata": {},
   "source": [
    "When you fit a VotingClassifier, it clones every estimator and fits the clones. The\n",
    "original estimators are available via the estimators attribute, while the fitted clones\n",
    "are available via the estimators_ attribute. If you prefer a dict rather than a list, you\n",
    "can use named_estimators or named_estimators_ instead. To begin, let’s look at each\n",
    "fitted classifier’s accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697d27fb-8a7c-42a0-bca0-b2f8a3c0deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca83046-c077-4b8c-a3df-69b24a9f8d00",
   "metadata": {},
   "source": [
    "When you call the voting classifier’s predict() method, it performs hard voting. For\n",
    "example, the voting classifier predicts class 1 for the first instance of the test set,\n",
    "because two out of three classifiers predict that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ce3b24-4f87-4026-8078-ea83840b65be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe48df3-9a35-4a41-9227-2239f42a8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1], dtype=int64), array([1], dtype=int64), array([0], dtype=int64)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830c5a5-b7ea-43e1-991c-e59fa131217b",
   "metadata": {},
   "source": [
    "Now let’s look at the performance of the voting classifier on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8d249c-0b06-4a3d-9c6c-15516d62ac2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff6a1a-8397-45a9-8c8c-bf0e20034931",
   "metadata": {},
   "source": [
    "There you have it! The voting classifier outperforms all the individual classifiers.\n",
    "If all classifiers are able to estimate class probabilities (i.e., if they all have a\n",
    "predict_proba() method), then you can tell Scikit-Learn to predict the class with\n",
    "the highest class probability, averaged over all the individual classifiers. This is called\n",
    "soft voting. It often achieves higher performance than hard voting because it gives\n",
    "more weight to highly confident votes. All you need to do is set the voting class\n",
    "fier’s voting hyperparameter to \"soft\", and ensure that all classifiers can estimate\n",
    "class probabilities. This is not the case for the SVC class by default, so you need\n",
    "to set its probability hyperparameter to True (this will make the SVC class use\n",
    "cross-validation to estimate class probabilities, slowing down training, and it will add\n",
    "a predict_proba() method). Let’s try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4548c9-7e3d-46c3-98ca-1065882de9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227446bb-ad3a-4c75-9cfd-d4a2db5ac15c",
   "metadata": {},
   "source": [
    "We reach 92% accuracy simply by using soft voting—not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c1ee-79ce-4db8-aad6-b9d0e7667aa3",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a279f7-a4f4-489b-8dbe-3baeeabb477d",
   "metadata": {},
   "source": [
    "Scikit-Learn offers a simple API for both bagging and pasting: BaggingClassifier\n",
    "class (or BaggingRegressor for regression). The following code trains an ensemble\n",
    "of 500 decision tree classifiers:6 each is trained on 100 training instances randomly\n",
    "sampled from the training set with replacement (this is an example of bagging, but\n",
    "if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\n",
    "tells Scikit-Learn the number of CPU cores to use for training and predictions, and\n",
    "–1 tells Scikit-Learn to use all available cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b46e2-bf24-4d6b-813b-35a52b835dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74c5d6-c3ad-4c06-8395-05af11a78abc",
   "metadata": {},
   "source": [
    "* A BaggingClassifier automatically performs soft voting instead\n",
    "of hard voting if the base classifier can estimate class probabilities\n",
    "(i.e., if it has a predict_proba() method), which is the case with\n",
    "decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9863ed-b768-4ef7-8537-fea9eae2e9cf",
   "metadata": {},
   "source": [
    "Figure below compares the decision boundary of a single decision tree with the decision\n",
    "boundary of a bagging ensemble of 500 trees (from the preceding code), both trained\n",
    "on the moons dataset. As you can see, the ensemble’s predictions will likely generalize\n",
    "much better than the single decision tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the\n",
    "training set, but the decision boundary is less irregular).\n",
    "Bagging introduces a bit more diversity in the subsets that each predictor is trained\n",
    "on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity\n",
    "also means that the predictors end up being less correlated, so the ensemble’s variance\n",
    "is reduced. Overall, bagging often results in better models, which explains why\n",
    "it’s generally preferred. But if you have spare time and CPU power, you can use\n",
    "cross-validation to evaluate both bagging and pasting and select the one that works\n",
    "best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a06601-be7d-41f7-b919-d5959fed56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code - To generate figure below\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
    "    axes=[-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
    "    colors = [\"#78785c\", \"#c47b27\"]\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X_train, y_train)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X_train, y_train)\n",
    "plt.title(\"Decision Trees with Bagging\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabe73e-e062-4253-989a-92b5bc5d0984",
   "metadata": {},
   "source": [
    "## Out-of-Bag evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69668566-2187-4898-bfa9-5160596fa1fc",
   "metadata": {},
   "source": [
    "With bagging, some training instances may be sampled several times for any given\n",
    "predictor, while others may not be sampled at all. By default a BaggingClassifier\n",
    "samples m training instances with replacement (bootstrap=True), where m is the\n",
    "size of the training set. With this process, it can be shown mathematically that only\n",
    "about 63% of the training instances are sampled on average for each predictor.7 The\n",
    "remaining 37% of the training instances that are not sampled are called out-of-bag\n",
    "(OOB) instances. Note that they are not the same 37% for all predictors.\n",
    "A bagging ensemble can be evaluated using OOB instances, without the need for\n",
    "a separate validation set: indeed, if there are enough estimators, then each instance\n",
    "in the training set will likely be an OOB instance of several estimators, so these\n",
    "estimators can be used to make a fair ensemble prediction for that instance. Once\n",
    "you have a prediction for each instance, you can compute the ensemble’s prediction\n",
    "accuracy (or any other metric).\n",
    "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier\n",
    "to request an automatic OOB evaluation after training. The following code demonstrates this. The resulting evaluation score is available in the oob_score_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec2172-9ddc-41f9-8cd0-f1bcfbb1ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            oob_score=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e02d7-c406-43a7-9e2f-6c5bcdc7f3f7",
   "metadata": {},
   "source": [
    "According to this OOB evaluation, this BaggingClassifier is likely to achieve about\n",
    "89.6% accuracy on the test set. Let’s verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b170953-d91f-4a0b-aeeb-bb081cb11126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc2009-d270-472f-8fe6-1485321bdf25",
   "metadata": {},
   "source": [
    "We get 92% accuracy on the test. The OOB evaluation was a bit too pessimistic, just\n",
    "over 2% too low.\n",
    "The OOB decision function for each training instance is also available through the\n",
    "oob_decision_function_ attribute. Since the base estimator has a predict_proba()\n",
    "method, the decision function returns the class probabilities for each training\n",
    "instance. For example, the OOB evaluation estimates that the first training instance\n",
    "has a 67.6% probability of belonging to the positive class and a 32.4% probability of\n",
    "belonging to the negative class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a884f-c10e-4f94-a4c2-7b0b4cf89fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f36b7b-a9c6-4a5f-99ca-0185d5c4205c",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ebad7-ebd9-494d-9dd6-ed83ff6d852e",
   "metadata": {},
   "source": [
    "a random forest10 is an ensemble of decision trees, generally\n",
    "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
    "set to the size of the training set. Instead of building a BaggingClassifier and\n",
    "passing it a DecisionTreeClassifier, you can use the RandomForestClassifier\n",
    "class, which is more convenient and optimized for decision trees11 (similarly, there\n",
    "is a RandomForestRegressor class for regression tasks). The following code trains a\n",
    "random forest classifier with 500 trees, each limited to maximum 16 leaf nodes, using\n",
    "all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c06ab-cd64-442e-a9b7-316983616739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n",
    "                                 n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75948a-ed21-46c9-a5de-d511b1ccb0b1",
   "metadata": {},
   "source": [
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of\n",
    "a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.\n",
    "The random forest algorithm introduces extra randomness when growing trees;\n",
    "instead of searching for the very best feature when splitting a node (see Chapter 6),\n",
    "it searches for the best feature among a random subset of features. By default, it\n",
    "samples nfeatures (where n is the total number of features). The algorithm results\n",
    "in greater tree diversity, which (again) trades a higher bias for a lower variance,\n",
    "generally yielding an overall better model. So, the following BaggingClassifier is\n",
    "equivalent to the previous RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24d606-666e-4494-8a8a-430ee2e15a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "    n_estimators=500, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af059822-4bc8-4775-8bc7-e0600a2a73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – verifies that the predictions are identical\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "np.all(y_pred_bag == y_pred_rf)  # same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd2044-a975-4ff4-bf39-55a47b7ab3f6",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d1d89-f8ad-460e-abb1-ecfa9fd04b16",
   "metadata": {},
   "source": [
    "Yet another great quality of random forests is that they make it easy to measure the\n",
    "relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average,\n",
    "across all trees in the forest. More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it\n",
    "(see Week 7 Lecture Notes).\n",
    "Scikit-Learn computes this score automatically for each feature after training, then\n",
    "it scales the results so that the sum of all importances is equal to 1. You can access\n",
    "the result using the feature_importances_ variable. For example, the following code\n",
    "trains a RandomForestClassifier on the iris dataset and\n",
    "outputs each feature’s importance. It seems that the most important features are\n",
    "the petal length (44%) and width (42%), while sepal length and width are rather\n",
    "unimportant in comparison (11% and 2%, respectively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa62c13-c33e-4095-b9b9-a4fdfd5c20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score, 2), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e57141-92c9-4f71-a171-7cbbcaf2770d",
   "metadata": {},
   "source": [
    "Similarly, if you train a random forest classifier on the MNIST dataset and plot each pixel’s importance, you get the image represented in figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527dd4c-8fb3-4406-9206-5dfc5d39c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates figure below\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False,\n",
    "                                parser='auto')\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(X_mnist, y_mnist)\n",
    "\n",
    "heatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\n",
    "plt.imshow(heatmap_image, cmap=\"hot\")\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n",
    "                           rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df742f8-262d-4ccb-a53f-5f1e02b87093",
   "metadata": {},
   "source": [
    "Random forests are very handy to get a quick understanding of what features actually\n",
    "matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4edbc8-9181-45ff-acce-77199240c97e",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03f397-f0dc-44f3-a5b4-ff9d12449e3e",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667d350-deec-484f-a631-9560d0262efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates figure below\n",
    "\n",
    "m = len(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
    "    sample_weights = np.ones(m) / m\n",
    "    plt.sca(axes[subplot])\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "\n",
    "        error_weights = sample_weights[y_pred != y_train].sum()\n",
    "        r = error_weights / sample_weights.sum()  # equation 7-1\n",
    "        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n",
    "        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n",
    "        sample_weights /= sample_weights.sum()  # normalization step\n",
    "\n",
    "        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n",
    "        plt.title(f\"learning_rate = {learning_rate}\")\n",
    "    if subplot == 0:\n",
    "        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
    "        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
    "        plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
    "        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
    "        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e015ce5-c08c-49c0-b9de-6fe3b2c19155",
   "metadata": {},
   "source": [
    "The following code trains an AdaBoost classifier based on 30 decision stumps using\n",
    "Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an\n",
    "AdaBoostRegressor class). A decision stump is a decision tree with max_depth=1—in\n",
    "other words, a tree composed of a single decision node plus two leaf nodes. This is\n",
    "the default base estimator for the AdaBoostClassifier class:\n",
    "\n",
    "Notes: If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regularizing the base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7d613-d0ce-48c9-b6bf-d0708f6636bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e23c8d-4373-4d74-bbe7-f7f86e46c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – in case you're curious to see what the decision boundary\n",
    "#              looks like for the AdaBoost classifier\n",
    "plot_decision_boundary(ada_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa0578-162f-4f63-9122-bca5d4c81faa",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a96a12-c6d5-4f87-ab89-027b2c7fc0d2",
   "metadata": {},
   "source": [
    "Another very popular boosting algorithm is gradient boosting.17 Just like AdaBoost,\n",
    "gradient boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor.\n",
    "\n",
    "Let’s go through a simple regression example, using decision trees as the base predic‐\n",
    "tors; this is called gradient tree boosting, or gradient boosted regression trees (GBRT).\n",
    "First, let’s generate a noisy quadratic dataset and fit a DecisionTreeRegressor to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586f5fc-4444-4f4b-8925-55172e14d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b96458-e610-47b8-96d4-f0fe9c98289b",
   "metadata": {},
   "source": [
    "Now let's train another decision tree regressor on the residual errors made by the previous predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f17ba-e307-4041-adcb-f754f6c087f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1e12d-afe8-4f9c-9374-8fafae15731e",
   "metadata": {},
   "source": [
    "And then we’ll train a third regressor on the residual errors made by the second\n",
    "predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8c281-9af8-492c-a082-a022b56bd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e00d2-49e5-413a-85af-c162cf97d813",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c372529-38da-40ac-937a-1fe70fa002f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba593d2f-87f2-4e05-8396-6cedcbc95c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates figure below\n",
    "\n",
    "def plot_predictions(regressors, X, y, axes, style,\n",
    "                     label=None, data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n",
    "                 for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\")\n",
    "    plt.axis(axes)\n",
    "\n",
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
    "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.title(\"Ensemble predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1)$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n",
    "                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n",
    "                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f116f-3737-4563-9c58-85b6995b8b97",
   "metadata": {},
   "source": [
    "You can use Scikit-Learn’s GradientBoostingRegressor class to train GBRT ensembles more easily (there’s also a GradientBoostingClassifier class for classification). Much like the RandomForestRegressor class, it has hyperparameters to\n",
    "control the growth of decision trees (e.g., max_depth, min_samples_leaf), as well\n",
    "as hyperparameters to control the ensemble training, such as the number of trees\n",
    "(n_estimators). The following code creates the same ensemble as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2602963-d057-4260-b16d-ede9df9e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
    "                                 learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde50ee0-4555-42ee-b86e-4e71ae31b7c1",
   "metadata": {},
   "source": [
    "To find the optimal number of trees, you could perform cross-validation using\n",
    "GridSearchCV or RandomizedSearchCV, as usual, but there’s a simpler way: if you set\n",
    "the n_iter_no_change hyperparameter to an integer value, say 10, then the Gradient\n",
    "BoostingRegressor will automatically stop adding more trees during training if it\n",
    "sees that the last 10 trees didn’t help. This is simply early stopping (introduced in\n",
    "Chapter 4), but with a little bit of patience: it tolerates having no progress for a few\n",
    "iterations before it stops. Let’s train the ensemble using early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde9e3c-36a3-4332-bb91-f24e7041d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
    "    n_iter_no_change=10, random_state=42)\n",
    "gbrt_best.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f77d6f-fddd-4810-a95f-fd209d640844",
   "metadata": {},
   "source": [
    "If you set n_iter_no_change too low, training may stop too early and the model will\n",
    "underfit. But if you set it too high, it will overfit instead. We also set a fairly small\n",
    "learning rate and a high number of estimators, but the actual number of estimators in\n",
    "the trained ensemble is much lower, thanks to early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee733e09-f805-49d3-8f4a-1fe57eaaaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best.n_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287ea2-9e33-4bcd-a1c9-cc3734e8edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates figure below\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
    "                 label=\"Ensemble predictions\")\n",
    "plt.title(f\"learning_rate={gbrt.learning_rate}, \"\n",
    "          f\"n_estimators={gbrt.n_estimators_}\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n",
    "plt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n",
    "          f\"n_estimators={gbrt_best.n_estimators_}\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c0ab4-191f-4ce3-b246-a2052f54cff9",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ccd36e-98af-4b8d-be5e-45c7d8fe24af",
   "metadata": {},
   "source": [
    "Scikit-Learn provides two classes for stacking ensembles: StackingClassifier and\n",
    "StackingRegressor. For example, we can replace the VotingClassifier we used at\n",
    "the beginning of this chapter on the moons dataset with a StackingClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd3bda-024d-41e2-acd3-cca04634386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5  # number of cross-validation folds\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4f256-1279-4533-aed7-40e6a5cded69",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ef450-ca27-4c1a-b188-8f81afc810a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
